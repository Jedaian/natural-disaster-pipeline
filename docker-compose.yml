services:
  nifi:
    image: apache/nifi:latest
    container_name: nifi
    ports:
      - "8443:8443" #Port for the Nifi Web UI
    environment:
      - NIFI_WEB_HTTPS_PORT=8443
      - NIFI_WEB_HTTPS_HOST=0.0.0.0
      - SINGLE_USER_CREDENTIALS_USERNAME=admin
      - SINGLE_USER_CREDENTIALS_PASSWORD=adminadminadmin
    volumes:
      - nifi_conf:/opt/nifi/nifi-current/conf
      - nifi_logs:/opt/nifi/nifi-current/logs
      - nifi_data:/opt/nifi/nifi-current/database_repository
      - nifi_flowfile:/opt/nifi/nifi-current/flowfile_repository
      - nifi_content:/opt/nifi/nifi-current/content_repository
      - nifi_provenance:/opt/nifi/nifi-current/provenance_repository
    networks:
      - pipeline-network
  
  redpanda:
    image: docker.redpanda.com/redpandadata/redpanda:latest
    container_name: redpanda
    command: 
      - redpanda
      - start
      - --kafka-addr internal://0.0.0.0:9092,external://0.0.0.0:19092
      - --advertise-kafka-addr internal://redpanda:9092,external://localhost:19092
      - --pandaproxy-addr internal://0.0.0.0:8082,external://0.0.0.0:18082
      - --advertise-pandaproxy-addr internal://redpanda:8082,external://localhost:18082
      - --schema-registry-addr internal://0.0.0.0:8081,external://0.0.0.0:18081
      - --rpc-addr redpanda:33145
      - --advertise-rpc-addr redpanda:33145
      - --mode dev-container
      - --smp 1
      - --memory 1G
      - --reserve-memory 0M
      - --overprovisioned
      - --default-log-level=info
    ports:
      - "18081:18081" #Schema Registry
      - "18082:18082" #Panda Proxy
      - "19092:19092" #Kafka API
      - "19644:9644"  #Admin API
    networks:
      - pipeline-network
    healthcheck:
      test: ["CMD-SHELL", "rpk cluster health | grep -E 'Healthy:.true+' || exit 1"]
      interval: 15s
      timeout: 3s
      retries: 5
  
  redpanda-console:
    image: docker.redpanda.com/redpandadata/console:latest
    container_name: redpanda-console
    environment:
      KAFKA_BROKERS: redpanda:9092
      KAFKA_SCHEMAREGISTRY_ENABLED: "true"
      KAFKA_SCHEMAREGISTRY_URLS: http://redpanda:8081
      REDPANDA_ADMINAPI_ENABLED: "true"
      REDPANDA_ADMINAPI_URLS: http://redpanda:9644
    ports:
      - "8090:8080" #External port:internal port
    networks:
      - pipeline-network
    depends_on:
      - redpanda

  spark-master:
    image: apache/spark:3.4.0
    container_name: spark-master
    command: >
      bash -c "
      /opt/spark/sbin/start-master.sh &&
      tail -f /opt/spark/logs/spark--org.apache.spark.deploy.master.Master-*.out
      "
    environment:
      - SPARK_NO_DAEMONIZE=true
    ports:
      - "8070:8080" #Spark Webserver UI
      - "7077:7077" #Spark Master Port
    volumes:
      - ./spark-apps:/opt/spark-apps
      - ./spark-data:/opt/spark-data
    networks:
      - pipeline-network

  spark-worker:
    image: apache/spark:3.4.0
    container_name: spark-worker
    command: >
      bash -c "
      /opt/spark/sbin/start-worker.sh spark://spark-master:7077 &&
      tail -f /opt/spark/logs/spark--org.apache.spark.deploy.worker.Worker-*.out
      "
    environment:
      - SPARK_NO_DAEMONIZE=true
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    volumes:
      - ./spark-apps:/opt/spark-apps
      - ./spark-data:/opt/spark-data
    networks:
      - pipeline-network
    depends_on:
      - spark-master

  dbt:
    image: python:3.11-slim
    container_name: dbt
    working_dir: /usr/app/dbt
    volumes:
      - ./dbt:/usr/app/dbt
      - ./spark-data:/usr/app/data
      - ./scripts:/usr/app/scripts
      - ./config/gcp-credentials.json:/usr/app/gcp-credentials.json:ro
    networks:
      - pipeline-network
    environment:
      - GOOGLE_APPLICATION_CREDENTIALS=/usr/app/gcp-credentials.json
      - GCS_BUCKET=natural-events-staging
      - BQ_PROJECT=natural-events-pipeline
      - BQ_DATASET=natural_events
      - DUCKDB_PATH=/usr/app/data/natural_events.duckdb
    entrypoint: ["/bin/bash", "-c", "pip install dbt-core dbt-duckdb google-cloud-storage google-cloud-bigquery pandas pyarrow duckdb && tail -f /dev/null"]

  airflow-init:
    build: .
    container_name: airflow-init
    entrypoint: /bin/bash
    command: -c "
      echo 'Initializing Airflow DB (SQLite)...'; \
      airflow db migrate && \
      echo 'Creating default user...'; \
      airflow users create \
        --username admin \
        --password admin \
        --firstname Admin \
        --lastname User \
        --role Admin \
        --email admin@example.com || echo 'User already exists'; \
      airflow connections add 'duckdb_default' \
        --conn-type 'duckdb' \
        --conn-host '/opt/spark-data/natural_events.duckdb' || echo 'Connection already exists';
      "
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./spark-data:/opt/spark-data
      - ./dbt:/opt/dbt
      - ./scripts:/opt/scripts
      - ./config/gcp-credentials.json:/opt/config/gcp-credentials.json:ro
    networks:
      - pipeline-network
    restart: on-failure

  airflow-webserver:
    build: .
    container_name: airflow-webserver
    command: webserver
    ports:
      - "8080:8080"
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      GOOGLE_APPLICATION_CREDENTIALS: /opt/config/gcp-credentials.json
      GCS_BUCKET: natural-events-staging
      BQ_PROJECT: natural-events-pipeline
      BQ_DATASET: natural_events
      DUCKDB_PATH: /opt/spark-data/natural_events.duckdb
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./spark-data:/opt/spark-data
      - ./dbt:/opt/dbt
      - ./scripts:/opt/scripts
      - ./config/gcp-credentials.json:/opt/config/gcp-credentials.json:ro
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    networks:
      - pipeline-network
    restart: always

  airflow-scheduler:
    build: .
    container_name: airflow-scheduler
    command: scheduler
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      GOOGLE_APPLICATION_CREDENTIALS: /opt/config/gcp-credentials.json
      GCS_BUCKET: natural-events-staging
      BQ_PROJECT: natural-events-pipeline
      BQ_DATASET: natural_events
      DUCKDB_PATH: /opt/spark-data/natural_events.duckdb
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./spark-data:/opt/spark-data
      - ./dbt:/opt/dbt
      - ./scripts:/opt/scripts
      - ./config/gcp-credentials.json:/opt/config/gcp-credentials.json:ro
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    networks:
      - pipeline-network
    restart: always


volumes:
  nifi_conf:
  nifi_logs:
  nifi_data:
  nifi_flowfile:
  nifi_content:
  nifi_provenance:
  postgres-data:
  logs:

networks:
  pipeline-network:
    driver: bridge